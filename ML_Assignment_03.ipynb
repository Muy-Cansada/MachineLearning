{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9lgQN6BSym6F"
      },
      "outputs": [],
      "source": [
        "# 17-04-2024\n",
        "# CSC354 – Assignment3 – ML – Support Vector Machines\n",
        "# Hamna Shahbaz\n",
        "# FA21-BSE-048\n",
        "# Using SVM to classify datapoints and evaluating best models\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.\n",
        "1. It is not possible to properly fit the dataset into two different classes without any label for either class.\n",
        "We cannot determine which datapoint belongs to which class wihout any labelled classification. Thus, it is not possible to fit the model onto an unlablled dataset and distinguish its negative and positive instances.\n",
        "The following code would fit the model and divide the datapoints in different points if there were labels for 'X' and 'Y' columns."
      ],
      "metadata": {
        "id": "-uR8ao2B2k73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 1.1\n",
        "dataset = pd.read_csv(\"dataset-q-1.csv\")\n",
        "\n",
        "X = dataset.drop(columns=['label'])\n",
        "y = dataset['label']\n",
        "\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X, y)\n"
      ],
      "metadata": {
        "id": "ZFDmt023z9_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1.\n",
        "2. It is possible to somewhat spot outliers by using a scatter plot and a decision boundary by distinguishing the points that are in far away corners of the plot. The decision boundary will just be there to separate the area of the model and would not actually have any signifcant meaning as there will be no classification task. The given code would plot the scatter plot across a boundary in an unnamed, dummy lablled column."
      ],
      "metadata": {
        "id": "TWAQFn647IMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 1.2\n",
        "dataset = pd.read_csv(\"dataset-q-1.csv\")\n",
        "\n",
        "X = dataset.drop(columns=['Unnamed: 0'])\n",
        "y_dummy = np.zeros(len(X))\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X, y_dummy)\n",
        "\n",
        "# Function to plot decision boundary\n",
        "def plot_decision_boundary(X, model):\n",
        "    plt.scatter(X.iloc[:, 0], X.iloc[:, 1], color='blue', label='Data Points')\n",
        "\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
        "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
        "    YY, XX = np.meshgrid(yy, xx)\n",
        "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
        "    Z = model.decision_function(xy).reshape(XX.shape)\n",
        "\n",
        "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
        "    plt.xlabel(X.columns[0])\n",
        "    plt.ylabel(X.columns[1])\n",
        "    plt.title('Decision Boundary and Data Points')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary(X, svm)\n"
      ],
      "metadata": {
        "id": "cKzmh3k070A6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 03\n",
        "#Loading the dataset and fitting it onto the model\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "# Use the first two features\n",
        "X = iris.data[:, :2]\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Polynomial kernel (degree = 2)\n",
        "svm_poly = SVC(kernel='poly', degree=2)\n",
        "svm_poly.fit(X_train, y_train)\n",
        "\n",
        "# Gaussian kernel (sigma = 1)\n",
        "svm_gaussian = SVC(kernel='rbf', gamma=1)\n",
        "svm_gaussian.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the performance of each model\n",
        "poly_accuracy = accuracy_score(y_test, svm_poly.predict(X_test))\n",
        "gaussian_accuracy = accuracy_score(y_test, svm_gaussian.predict(X_test))\n",
        "\n",
        "print(\"Accuracy with polynomial kernel (degree = 2):\", poly_accuracy)\n",
        "print(\"Accuracy with Gaussian kernel (sigma = 1):\", gaussian_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaEdA1Yf9b4O",
        "outputId": "72385632-f120-4ddb-9dc0-d415cc5bfa79"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with polynomial kernel (degree = 2): 0.8333333333333334\n",
            "Accuracy with Gaussian kernel (sigma = 1): 0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 03\n",
        "#Creating a function for  evaluating the performance of both kernels\n",
        "def evaluate_svm(X_train, X_test, y_train, y_test, kernel, param1_name, param1_values, param2_name=None, param2_values=None):\n",
        "    results = []\n",
        "    for param1_value in param1_values:\n",
        "        if param2_values:\n",
        "            for param2_value in param2_values:\n",
        "                if kernel == 'poly':\n",
        "                    model = SVC(kernel='poly', degree=param1_value, C=param2_value)\n",
        "                elif kernel == 'rbf':\n",
        "                    model = SVC(kernel='rbf', gamma=param1_value, C=param2_value)\n",
        "                model.fit(X_train, y_train)\n",
        "                accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "                results.append((param1_name, param1_value, param2_name, param2_value, accuracy))\n",
        "        else:\n",
        "            if kernel == 'poly':\n",
        "                model = SVC(kernel='poly', degree=param1_value)\n",
        "            elif kernel == 'rbf':\n",
        "                model = SVC(kernel='rbf', gamma=param1_value)\n",
        "            model.fit(X_train, y_train)\n",
        "            accuracy = accuracy_score(y_test, model.predict(X_test))\n",
        "            results.append((param1_name, param1_value, accuracy))\n",
        "    return results\n",
        "\n",
        "# Define parameter values to try\n",
        "C_values = [0.1, 1, 10]\n",
        "degree_values = [1, 2, 3]\n",
        "sigma_values = [0.1, 1, 10]\n",
        "\n",
        "# Task 2: Vary both C and degree for polynomial kernel\n",
        "poly_results = evaluate_svm(X_train, X_test, y_train, y_test, kernel='poly', param1_name='degree', param1_values=degree_values, param2_name='C', param2_values=C_values)\n",
        "print(\"Polynomial Kernel Results:\")\n",
        "for result in poly_results:\n",
        "    print(result)\n",
        "\n",
        "# Task 3: Vary both C and sigma for Gaussian kernel\n",
        "gaussian_results = evaluate_svm(X_train, X_test, y_train, y_test, kernel='rbf', param1_name='sigma', param1_values=sigma_values, param2_name='C', param2_values=C_values)\n",
        "print(\"\\nGaussian Kernel Results:\")\n",
        "for result in gaussian_results:\n",
        "    print(result)\n"
      ],
      "metadata": {
        "id": "H4UPKh_v-RSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3.\n",
        "**Findings**\n",
        "\n",
        "*   Varying C value:  We tried three different values of C: 0.1, 1, and 10. As C increases from 0.1 to 1 and then to 10, the accuracy of the SVM model may increase if the model doesn't overfit.\n",
        "*   Varying Sigma value:  As sigma increases from 0.1 to 1 and then to 10, the decision boundary might become more complex, but it could also lead to overfitting if not properly controlled.\n",
        "\n",
        "*   Varying C value: Increasing C generally results in more complex decision boundaries, which can better fit the training data but may not generalize well to unseen data.\n",
        "*   Varying Degree value: We explored three different values of degree: 1, 2, and 3. As the degree increases, the decision boundary becomes more flexible and can fit more complex patterns in the data.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1OoCtbxB-5Im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "dataset = pd.read_csv(\"dataset-q-4.csv\")\n",
        "\n",
        "# Separate features (X) and labels (y)\n",
        "X = dataset.drop(columns=['label'])\n",
        "y = dataset['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "param_grid = {'C': [0.1, 1, 10],\n",
        "              'gamma': [0.1, 1, 10]}\n",
        "svm = SVC(kernel='rbf')\n",
        "\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "optimal_C = grid_search.best_params_['C']\n",
        "optimal_sigma = grid_search.best_params_['gamma']\n",
        "\n",
        "optimal_svm = SVC(kernel='rbf', C=optimal_C, gamma=optimal_sigma)\n",
        "optimal_svm.fit(X_train, y_train)\n",
        "\n",
        "y_pred = optimal_svm.predict(X_test)\n",
        "evaluation_result = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Optimal value of C:\", optimal_C)\n",
        "print(\"Optimal value of sigma:\", optimal_sigma)\n",
        "print(\"Evaluation result (accuracy) on the testing data:\", evaluation_result)\n",
        "\n"
      ],
      "metadata": {
        "id": "3jOxKvKoABtC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}