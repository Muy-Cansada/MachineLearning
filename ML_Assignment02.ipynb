{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 27-03-2024\n",
        "# CSC354 – Assignmen2 – ML – Decision Trees\n",
        "# Hamna Shahbaz\n",
        "# FA21-BSE-048\n",
        "# Performing classification and regression tasks using decision trees and generating a report about them."
      ],
      "metadata": {
        "id": "acGpOOVdRi2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "m5FLP8xQmVxU",
        "outputId": "bcf6980d-f18d-439d-ffb1-38be51baa26e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J48 Baseline Accuracy: 0.3972972972972973\n",
            "Random Forest Baseline Accuracy: 0.46216216216216216\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py:269: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features='sqrt'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J48 Best Parameters (Random Search): {'splitter': 'best', 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'auto', 'max_depth': 40, 'criterion': 'entropy'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-62454ac1ede4>\u001b[0m in \u001b[0;36m<cell line: 60>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mrf_grid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_param_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mrf_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0mrf_best_params_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_grid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random Forest Best Parameters (Grid Search):\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_best_params_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1705\u001b[0m                 (self._jobs[0].get_status(\n\u001b[1;32m   1706\u001b[0m                     timeout=self.timeout) == TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Question 01:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "dataset = pd.read_csv('/datasaurus.csv')\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop(columns=['dataset'])\n",
        "y = dataset['dataset']\n",
        "\n",
        "# Train-test Split (80% training, 20% testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Baseline model with default parameters: J48 (DecisionTreeClassifier)\n",
        "j48_classifier = DecisionTreeClassifier(random_state=42)\n",
        "j48_classifier.fit(X_train, y_train)\n",
        "j48_predictions_baseline = j48_classifier.predict(X_test)\n",
        "j48_accuracy_baseline = accuracy_score(y_test, j48_predictions_baseline)\n",
        "print(\"J48 Baseline Accuracy:\", j48_accuracy_baseline)\n",
        "\n",
        "# Baseline model with default parameters: Random Forest\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "rf_predictions_baseline = rf_classifier.predict(X_test)\n",
        "rf_accuracy_baseline = accuracy_score(y_test, rf_predictions_baseline)\n",
        "print(\"Random Forest Baseline Accuracy:\", rf_accuracy_baseline)\n",
        "\n",
        "# Hyperparameter tuning using Randomized Search for J48\n",
        "j48_param_dist = {\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"splitter\": [\"best\", \"random\"],\n",
        "    \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
        "    \"min_samples_split\": [2, 5, 10, 20],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
        "}\n",
        "j48_random_search = RandomizedSearchCV(j48_classifier, j48_param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
        "j48_random_search.fit(X_train, y_train)\n",
        "j48_best_params_random = j48_random_search.best_params_\n",
        "print(\"J48 Best Parameters (Random Search):\", j48_best_params_random)\n",
        "\n",
        "# Adjusting parameter search spaces for Grid Search (Random Forest)\n",
        "rf_param_grid = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"criterion\": [\"gini\", \"entropy\"],\n",
        "    \"max_depth\": [None, 10, 20],\n",
        "    \"min_samples_split\": [2, 5],\n",
        "    \"min_samples_leaf\": [1, 2],\n",
        "    \"max_features\": [\"auto\", \"sqrt\"]\n",
        "}\n",
        "\n",
        "rf_grid_search = GridSearchCV(rf_classifier, rf_param_grid, cv=5, n_jobs=-1)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "rf_best_params_grid = rf_grid_search.best_params_\n",
        "print(\"Random Forest Best Parameters (Grid Search):\", rf_best_params_grid)\n",
        "\n",
        "# Evaluate models with best parameters\n",
        "j48_best_classifier = DecisionTreeClassifier(**j48_best_params_random, random_state=42)\n",
        "j48_best_classifier.fit(X_train, y_train)\n",
        "j48_predictions_best = j48_best_classifier.predict(X_test)\n",
        "j48_accuracy_best = accuracy_score(y_test, j48_predictions_best)\n",
        "print(\"J48 Best Accuracy:\", j48_accuracy_best)\n",
        "\n",
        "rf_best_classifier = RandomForestClassifier(**rf_best_params_grid, random_state=42)\n",
        "rf_best_classifier.fit(X_train, y_train)\n",
        "rf_predictions_best = rf_best_classifier.predict(X_test)\n",
        "rf_accuracy_best = accuracy_score(y_test, rf_predictions_best)\n",
        "print(\"Random Forest Best Accuracy:\", rf_accuracy_best)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 02:\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "dataset = pd.read_csv('/cars-dataset.csv')\n",
        "\n",
        "# Check the column names in the dataset\n",
        "print(dataset.columns)\n",
        "\n",
        "# Split the dataset into features and target variable\n",
        "X = dataset.drop(columns=['selling_price'])  # Features\n",
        "y = dataset['selling_price']  # Target variable\n",
        "\n",
        "# Preprocess categorical variables using one-hot encoding to avoid ValueErrors\n",
        "X_encoded = pd.get_dummies(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt_regressor_baseline = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor_baseline.fit(X_train, y_train)\n",
        "dt_predictions_baseline = dt_regressor_baseline.predict(X_test)\n",
        "mse_baseline = mean_squared_error(y_test, dt_predictions_baseline)\n",
        "print(\"Baseline Mean Squared Error:\", mse_baseline)\n",
        "\n",
        "# Hyperparameter tuning using Randomized Search\n",
        "dt_param_dist = {\n",
        "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
        "    \"splitter\": [\"best\", \"random\"],\n",
        "    \"max_depth\": [None, 10, 20, 30, 40, 50],\n",
        "    \"min_samples_split\": [2, 5, 10],\n",
        "    \"min_samples_leaf\": [1, 2, 4],\n",
        "    \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
        "}\n",
        "\n",
        "dt_random_search = RandomizedSearchCV(dt_regressor_baseline, dt_param_dist, n_iter=100, cv=5, random_state=42, n_jobs=-1)\n",
        "dt_random_search.fit(X_train, y_train)\n",
        "best_params_random = dt_random_search.best_params_\n",
        "print(\"Best Parameters (Random Search):\", best_params_random)\n",
        "\n",
        "# Hyperparameter tuning using Grid Search\n",
        "dt_param_grid = {\n",
        "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
        "    \"splitter\": [\"best\", \"random\"],\n",
        "    \"max_depth\": [None, 10, 20, 30],\n",
        "    \"min_samples_split\": [2, 5],\n",
        "    \"min_samples_leaf\": [1, 2],\n",
        "    \"max_features\": [\"auto\", \"sqrt\"]\n",
        "}\n",
        "\n",
        "dt_grid_search = GridSearchCV(dt_regressor_baseline, dt_param_grid, cv=5, n_jobs=-1)\n",
        "dt_grid_search.fit(X_train, y_train)\n",
        "best_params_grid = dt_grid_search.best_params_\n",
        "print(\"Best Parameters (Grid Search):\", best_params_grid)\n",
        "\n",
        "# Evaluate model with best parameters\n",
        "dt_regressor_best_random = DecisionTreeRegressor(**best_params_random, random_state=42)\n",
        "dt_regressor_best_random.fit(X_train, y_train)\n",
        "dt_predictions_best_random = dt_regressor_best_random.predict(X_test)\n",
        "mse_best_random = mean_squared_error(y_test, dt_predictions_best_random)\n",
        "print(\"Best Mean Squared Error (Random Search):\", mse_best_random)\n",
        "\n",
        "dt_regressor_best_grid = DecisionTreeRegressor(**best_params_grid, random_state=42)\n",
        "dt_regressor_best_grid.fit(X_train, y_train)\n",
        "dt_predictions_best_grid = dt_regressor_best_grid.predict(X_test)\n",
        "mse_best_grid = mean_squared_error(y_test, dt_predictions_best_grid)\n",
        "print(\"Best Mean Squared Error (Grid Search):\", mse_best_grid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2C3WwlqUHycR",
        "outputId": "5713b184-9068-4cc0-948e-5098dfb9ad3b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['year', 'km_driven', 'fuel', 'seller_type', 'transmission', 'owner',\n",
            "       'selling_price'],\n",
            "      dtype='object')\n",
            "Baseline Mean Squared Error: 190042705342.0196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters (Random Search): {'splitter': 'best', 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 20, 'criterion': 'squared_error'}\n",
            "Best Parameters (Grid Search): {'criterion': 'friedman_mse', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'best'}\n",
            "Best Mean Squared Error (Random Search): 166372967605.11325\n",
            "Best Mean Squared Error (Grid Search): 165453664215.49405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py:277: FutureWarning: `max_features='auto'` has been deprecated in 1.1 and will be removed in 1.3. To keep the past behaviour, explicitly set `max_features=1.0'`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**QUestion 03:**\n",
        "In question 01 the manual viewing of data did not provide me with enough information so I was forced to learn the importance of data visualization; which showed that apparently  mathematically different data is so interconnected so as to be used to draw a whole shape. The hyperparametering of the datasaurus dataset consumed a great amount of time for it to execute properly due to the continuosly optimal parameter searching by random and gird searches. Different accuracy values were generated at each iteration of the hyperparametering that was used for exploration of the concept.\n",
        "In question 2, classification tasks are fairly familiar and easy for me but regression is complex and new as I haven't been able to pay much attention to it. I had to deal with a great deals of warnings to properly comprehend and understand the nature of dynamic hyperparametering for a regression task. Overall I had quite a bit of difficulty trying to learn and track the output and the datasets used in these questions."
      ],
      "metadata": {
        "id": "uBLCl9hGMGbE"
      }
    }
  ]
}